<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>VEDA AI Voice Test</title>
    <style>
        * { box-sizing: border-box; margin: 0; padding: 0; }
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #1a1a2e 0%, #16213e 100%);
            color: #fff;
            min-height: 100vh;
            padding: 2rem;
        }
        .container {
            max-width: 600px;
            margin: 0 auto;
        }
        h1 {
            text-align: center;
            margin-bottom: 2rem;
            font-size: 2rem;
            background: linear-gradient(90deg, #00d4ff, #7c3aed);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
        }
        .status {
            text-align: center;
            padding: 0.5rem;
            margin-bottom: 1rem;
            border-radius: 8px;
            background: rgba(255,255,255,0.1);
        }
        .status.connected { border-left: 4px solid #10b981; }
        .status.disconnected { border-left: 4px solid #ef4444; }
        .record-btn {
            display: block;
            width: 120px;
            height: 120px;
            margin: 2rem auto;
            border-radius: 50%;
            border: none;
            background: linear-gradient(135deg, #7c3aed, #db2777);
            color: white;
            font-size: 1rem;
            cursor: pointer;
            transition: all 0.3s ease;
            box-shadow: 0 10px 30px rgba(124, 58, 237, 0.4);
        }
        .record-btn:hover { transform: scale(1.05); }
        .record-btn.recording {
            animation: pulse 1s infinite;
            background: linear-gradient(135deg, #ef4444, #f97316);
        }
        @keyframes pulse {
            0%, 100% { box-shadow: 0 0 0 0 rgba(239, 68, 68, 0.7); }
            50% { box-shadow: 0 0 0 20px rgba(239, 68, 68, 0); }
        }
        .transcript, .response {
            background: rgba(255,255,255,0.05);
            border-radius: 12px;
            padding: 1rem;
            margin: 1rem 0;
            min-height: 60px;
        }
        .transcript h3, .response h3 {
            font-size: 0.875rem;
            color: #a1a1aa;
            margin-bottom: 0.5rem;
        }
        .transcript p, .response p {
            font-size: 1.1rem;
            line-height: 1.6;
        }
        .response { border-left: 4px solid #7c3aed; }
        audio { width: 100%; margin-top: 1rem; }
    </style>
</head>
<body>
    <div class="container">
        <h1>üé§ VEDA AI Voice</h1>
        
        <div id="status" class="status disconnected">Disconnected</div>
        
        <button id="recordBtn" class="record-btn">üéôÔ∏è Hold to Speak</button>
        
        <div class="transcript">
            <h3>üìù You said:</h3>
            <p id="transcriptText">Press and hold the button to speak in Hindi...</p>
        </div>
        
        <div class="response">
            <h3>ü§ñ VEDA says:</h3>
            <p id="responseText">Waiting for your question...</p>
        </div>
        
        <audio id="audioPlayer" controls style="display:none;"></audio>
    </div>

    <script>
        const WS_URL = 'ws://localhost:8000/api/v1/voice/ws';
        let ws;
        let mediaRecorder;
        let audioChunks = [];
        
        const statusEl = document.getElementById('status');
        const recordBtn = document.getElementById('recordBtn');
        const transcriptText = document.getElementById('transcriptText');
        const responseText = document.getElementById('responseText');
        const audioPlayer = document.getElementById('audioPlayer');
        
        // Connect WebSocket
        function connect() {
            ws = new WebSocket(WS_URL);
            
            ws.onopen = () => {
                statusEl.textContent = '‚úÖ Connected';
                statusEl.className = 'status connected';
            };
            
            ws.onclose = () => {
                statusEl.textContent = '‚ùå Disconnected - Reconnecting...';
                statusEl.className = 'status disconnected';
                setTimeout(connect, 3000);
            };
            
            ws.onerror = (e) => {
                console.error('WebSocket error:', e);
            };
            
            let audioBuffer = [];
            
            ws.onmessage = async (event) => {
                if (event.data instanceof Blob) {
                    // Audio chunk
                    const arrayBuffer = await event.data.arrayBuffer();
                    audioBuffer.push(new Uint8Array(arrayBuffer));
                } else {
                    const data = JSON.parse(event.data);
                    
                    if (data.type === 'transcript') {
                        transcriptText.textContent = data.text || '(empty)';
                    } else if (data.type === 'response') {
                        responseText.textContent = data.text || '(no response)';
                    } else if (data.type === 'audio_start') {
                        audioBuffer = [];
                    } else if (data.type === 'audio_end') {
                        // Combine and play audio
                        const blob = new Blob(audioBuffer, { type: 'audio/wav' });
                        audioPlayer.src = URL.createObjectURL(blob);
                        audioPlayer.style.display = 'block';
                        audioPlayer.play();
                    } else if (data.type === 'error') {
                        responseText.textContent = '‚ùå Error: ' + data.message;
                    }
                }
            };
        }
        
        // Recording logic
        recordBtn.addEventListener('mousedown', startRecording);
        recordBtn.addEventListener('mouseup', stopRecording);
        recordBtn.addEventListener('mouseleave', stopRecording);
        recordBtn.addEventListener('touchstart', startRecording);
        recordBtn.addEventListener('touchend', stopRecording);
        
        async function startRecording() {
            if (ws.readyState !== WebSocket.OPEN) return;
            
            try {
                const stream = await navigator.mediaDevices.getUserMedia({ 
                    audio: { sampleRate: 16000, channelCount: 1 } 
                });
                
                mediaRecorder = new MediaRecorder(stream, { mimeType: 'audio/webm' });
                audioChunks = [];
                
                mediaRecorder.ondataavailable = (e) => {
                    if (e.data.size > 0) {
                        audioChunks.push(e.data);
                    }
                };
                
                mediaRecorder.onstop = async () => {
                    const blob = new Blob(audioChunks, { type: 'audio/webm' });
                    const arrayBuffer = await blob.arrayBuffer();
                    ws.send(arrayBuffer);
                    ws.send(JSON.stringify({ type: 'end_stream' }));
                    stream.getTracks().forEach(t => t.stop());
                };
                
                mediaRecorder.start(100); // Collect data every 100ms
                recordBtn.classList.add('recording');
                recordBtn.textContent = 'üî¥ Recording...';
            } catch (err) {
                console.error('Mic access denied:', err);
                alert('Please allow microphone access');
            }
        }
        
        function stopRecording() {
            if (mediaRecorder && mediaRecorder.state === 'recording') {
                mediaRecorder.stop();
                recordBtn.classList.remove('recording');
                recordBtn.textContent = 'üéôÔ∏è Hold to Speak';
            }
        }
        
        // Start connection
        connect();
    </script>
</body>
</html>
